{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f366a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1517472000.0, '40.886127', '-73.81532']\n",
      "599\n",
      "400\n",
      "999\n",
      "あいうえお\n",
      "らりるれろ\n",
      "かきくけこ\n",
      "さしすせそ\n",
      "たちつてと\n",
      "なにぬねの\n",
      "はひふへほ\n",
      "まみむめも\n",
      "やゆよ\n",
      "わをん\n",
      "がぎぐげご\n",
      "ざじずぜぞ\n",
      "だぢづでど\n",
      "ばびぶべぼ\n",
      "ぱぴぷぺぽ\n",
      "っ\n",
      "アイウエオ\n",
      "ラリルレロ\n",
      "カキクケコ\n",
      "サシスセソ\n",
      "タチツテト\n",
      "ナニヌネノ\n",
      "ハヒフヘホ\n",
      "マミムメモ\n",
      "ヤユヨ\n",
      "ワヲン\n",
      "ガギグゲゴ\n",
      "ザジズゼゾ\n",
      "ダヂヅデド\n",
      "バビブベボ\n",
      "パピプペポ\n",
      "ッ\n",
      "日本語\n",
      "漢字\n",
      "ひらがな\n",
      "カタカナ\n",
      "アルファベット\n",
      "数字\n",
      "記号\n",
      "英語\n",
      "中国語\n",
      "韓国語\n",
      "MILF\n",
      "BOOBA\n",
      "YEEEHHHAAAWWW\n",
      "Using cpu device\n",
      "No model found, creating new model\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "599\n",
      "Training model\n",
      "loss: 30.809083  [   25/  599]\n",
      "Finished training model\n",
      "Testing model\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (25) must match the size of tensor b (5) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4008\\1810542220.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {t+1}\\n-------------------------------\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m     \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 232\u001b[1;33m     \u001b[0mtest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    233\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"model.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_4008\\1810542220.py\u001b[0m in \u001b[0;36mtest\u001b[1;34m(dataloader, model, loss_fn)\u001b[0m\n\u001b[0;32m    221\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m             \u001b[0mtest_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 223\u001b[1;33m             \u001b[0mcorrect\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    224\u001b[0m     \u001b[0mtest_loss\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0mnum_batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[0mcorrect\u001b[0m \u001b[1;33m/=\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (25) must match the size of tensor b (5) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import Data.database_handler as dbHandler\n",
    "from torchvision import transforms, utils\n",
    "import datetime as dt\n",
    "sys.path.append('..')\n",
    "#%run Map_grid/map.ipynb import CalculateGrid\n",
    "\n",
    "#Connecting to the SQLite database\n",
    "dataAmount = 200000\n",
    "dbPath = r'Data\\datasetNY.db'\n",
    "gridSize = 5\n",
    "chunkAmount = 1000\n",
    "chunkSize = dataAmount / chunkAmount\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, db_path, transform=None):\n",
    "        self.coordinates = dbHandler.get_n_data_datetime_converted(db_path, dataAmount)\n",
    "        self.coordinates = pd.DataFrame(self.coordinates, columns=['datetime', 'latitude', 'longitude'])\n",
    "        \n",
    "        #split into 500 chunks using numpy\n",
    "        self.coordinates = np.array_split(self.coordinates, chunkAmount)\n",
    "\n",
    "        #process each chunk and merge it back into one dataframe\n",
    "        self.grids = []\n",
    "        grid_lower_lat, grid_lower_long = 40.54, -74.15\n",
    "        grid_upper_lat, grid_upper_long = 40.91, -73.70\n",
    "        grid_lat_step = (grid_upper_lat - grid_lower_lat) / gridSize\n",
    "        grid_long_step = (grid_upper_long - grid_lower_long) / gridSize\n",
    "        for i in range(len(self.coordinates)-1):\n",
    "            grid = np.zeros((gridSize, gridSize))\n",
    "            for index, row in self.coordinates[i].iterrows():\n",
    "                coordinates = row['latitude'], row['longitude']\n",
    "                for j in range(gridSize):\n",
    "                    for k in range(gridSize):\n",
    "                        lat_lower = grid_lower_lat + j * grid_lat_step\n",
    "                        lat_upper = grid_lower_lat + (j + 1) * grid_lat_step\n",
    "                        long_lower = grid_lower_long + k * grid_long_step\n",
    "                        long_upper = grid_lower_long + (k + 1) * grid_long_step\n",
    "                        if lat_lower <= float(coordinates[0]) < lat_upper and long_lower <= float(coordinates[1]) < long_upper:\n",
    "                            grid[j][k] += 1\n",
    "                            break\n",
    "            self.grids.append(grid/chunkSize)\n",
    "        self.grids = np.array(self.grids)\n",
    "        self.transform = transform      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        grid = self.grids[idx]\n",
    "        grid = torch.from_numpy(grid).float()\n",
    "\n",
    "        max_index = np.argmax(grid)\n",
    "        max_index = np.array(max_index)\n",
    "        #get indicies of the highest value and nearest neighbours that are within 10% of the highest value\n",
    "        #max_index = np.unravel_index(max_index, grid.shape)\n",
    "        #max_index = np.array(max_index)\n",
    "        #max_index = max_index.flatten()\n",
    "        Jonasu_wa_kanji_o_motte_imasu = max_index.item() // gridSize, max_index.item() % gridSize\n",
    "        max_value = grid[Jonasu_wa_kanji_o_motte_imasu[0]][Jonasu_wa_kanji_o_motte_imasu[1]].item()\n",
    "        indicies = np.empty((0,2), float)\n",
    "        for i in range(gridSize):\n",
    "            for j in range(gridSize):\n",
    "                if grid[i][j].item() >= max_value * 0.9:\n",
    "                    indicies = np.append(indicies, i * gridSize + j)\n",
    "\n",
    "        indicies = torch.from_numpy(indicies).reshape(-1, 1).flatten()\n",
    "        indicies = torch.nn.functional.pad(indicies, (0, gridSize - len(indicies)), value=-1)\n",
    "        return grid, indicies\n",
    "\n",
    "accident_dataset = AccidentDataset(dbPath)\n",
    "\n",
    "#Create new array with 60% of the data\n",
    "train_size = int(0.6 * len(accident_dataset))\n",
    "test_size = len(accident_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(accident_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(accident_dataset))\n",
    "\n",
    "print('あいうえお') #aiueo\n",
    "print('らりるれろ') #rarirurero\n",
    "print('かきくけこ') #kakikukeko\n",
    "print('さしすせそ') #sashisuseso\n",
    "print('たちつてと') #tachitsuteto\n",
    "print('なにぬねの') #naninuneno\n",
    "print('はひふへほ') #hahifuheho\n",
    "print('まみむめも') #mamimumemo\n",
    "print('やゆよ') #yayuyo\n",
    "print('わをん') #wawon\n",
    "print('がぎぐげご') #gagigugego\n",
    "print('ざじずぜぞ') #zazizuzezo\n",
    "print('だぢづでど') #dazidizudezo\n",
    "print('ばびぶべぼ') #babibubebo\n",
    "print('ぱぴぷぺぽ') #papipupepo\n",
    "print('っ') #small tsu\n",
    "\n",
    "#now in katakana\n",
    "print('アイウエオ') #aiueo\n",
    "print('ラリルレロ') #rarirurero\n",
    "print('カキクケコ') #kakikukeko\n",
    "print('サシスセソ') #sashisuseso\n",
    "print('タチツテト') #tachitsuteto\n",
    "print('ナニヌネノ') #naninuneno\n",
    "print('ハヒフヘホ') #hahifuheho\n",
    "print('マミムメモ') #mamimumemo\n",
    "print('ヤユヨ') #yayuyo\n",
    "print('ワヲン') #wawon\n",
    "print('ガギグゲゴ') #gagigugego\n",
    "print('ザジズゼゾ') #zazizuzezo\n",
    "print('ダヂヅデド') #dazidizudezo\n",
    "print('バビブベボ') #babibubebo\n",
    "print('パピプペポ') #papipupepo\n",
    "print('ッ') #small tsu\n",
    "\n",
    "#now in kanji\n",
    "print('日本語') #nihongo\n",
    "print('漢字') #kanji\n",
    "print('ひらがな') #hiragana\n",
    "print('カタカナ') #katakana\n",
    "print('アルファベット') #alphabet\n",
    "print('数字') #number\n",
    "print('記号') #symbol\n",
    "print('英語') #english\n",
    "print('中国語') #chinese\n",
    "print('韓国語') #korean\n",
    "\n",
    "#Now in your mom\n",
    "print('MILF') #Jonas\n",
    "print('BOOBA') #NotJonas\n",
    "print('YEEEHHHAAAWWW') #Jonasu\n",
    "\n",
    "print('To fly is to controllably float using air as leverage, despite the gravitational pull of some giant mass') #JonasDescription\n",
    "\n",
    "#Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=gridSize*gridSize, shuffle=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=gridSize*gridSize, shuffle=False)\n",
    "\n",
    "# define the class for multilinear regression\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(gridSize*gridSize, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, gridSize),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# define the class for multilinear regression\n",
    "# building the model object\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = Network().to(device)\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    print(\"Loaded model from model.pth\")\n",
    "else:\n",
    "    print(\"No model found, creating new model\")\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define the training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    model.train()\n",
    "    print(\"Training model\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        # Compute prediction error\n",
    "        #print(X)\n",
    "        #print(y)\n",
    "        pred = model(X)\n",
    "        #print('pred: ', pred)\n",
    "        #print('y: ', y)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(\"Finished training model\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    print(\"Testing model\")\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "epochs = 5\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "570189b94c8be545687b2cc37d4a9df3fcede358db47b55e6620cb36780e1fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
