{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb9892de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1451478000.0, '40.640825', '-74.0785726']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import Data.database_handler as dbHandler\n",
    "from torchvision import transforms, utils\n",
    "import datetime as dt\n",
    "import random as rand\n",
    "sys.path.append('..')\n",
    "#%run Map_grid/map.ipynb import CalculateGrid\n",
    "\n",
    "#Connecting to the SQLite database\n",
    "data_amount = 1500000\n",
    "db_path = r'Data\\datasetNY.db'\n",
    "grid_size = 5\n",
    "chunk_amount = 225555\n",
    "chunk_size = data_amount / chunk_amount\n",
    "data = dbHandler.get_n_data_datetime_converted(db_path, data_amount)\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.coordinates = data\n",
    "        self.coordinates = pd.DataFrame(self.coordinates, columns=['datetime', 'latitude', 'longitude'])\n",
    "        \n",
    "        #split into 500 chunks using numpy\n",
    "        self.coordinates = np.array_split(self.coordinates, chunk_amount)\n",
    "\n",
    "        #process each chunk and merge it back into one dataframe\n",
    "        self.grids = []\n",
    "        grid_lower_lat, grid_lower_long = 40.54, -74.15\n",
    "        grid_upper_lat, grid_upper_long = 40.91, -73.70\n",
    "        grid_lat_step = (grid_upper_lat - grid_lower_lat) / grid_size\n",
    "        grid_long_step = (grid_upper_long - grid_lower_long) / grid_size\n",
    "        for i in range(len(self.coordinates)-1):\n",
    "            grid = np.zeros((grid_size, grid_size))\n",
    "            for index, row in self.coordinates[i].iterrows():\n",
    "                coordinates = row['latitude'], row['longitude']\n",
    "                for j in range(grid_size):\n",
    "                    for k in range(grid_size):\n",
    "                        lat_lower = grid_lower_lat + j * grid_lat_step\n",
    "                        lat_upper = grid_lower_lat + (j + 1) * grid_lat_step\n",
    "                        long_lower = grid_lower_long + k * grid_long_step\n",
    "                        long_upper = grid_lower_long + (k + 1) * grid_long_step\n",
    "                        if lat_lower <= float(coordinates[0]) < lat_upper and long_lower <= float(coordinates[1]) < long_upper:\n",
    "                            grid[j][k] += 1\n",
    "                            break\n",
    "            self.grids.append(grid/chunk_size)\n",
    "        self.grids = np.array(self.grids)\n",
    "        self.transform = transform      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        grid = self.grids[idx]\n",
    "        grid = torch.from_numpy(grid).float()\n",
    "\n",
    "        max_index = np.argmax(grid)\n",
    "        max_index = np.array(max_index)\n",
    "        return grid.flatten(), torch.tensor(max_index.item()).long()\n",
    "\n",
    "accident_dataset = AccidentDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f366a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135332\n",
      "90222\n",
      "225554\n",
      "Using cpu device\n",
      "Loaded model from model.pth\n",
      "Saved PyTorch Model State to model.pth\n",
      "42863\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "46447\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "22885\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "15019\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "27591\n",
      "Predicted: \"[11]\", Actual: \"6\"\n",
      "9194\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "47962\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "10220\n",
      "Predicted: \"[6, 0]\", Actual: \"18\"\n",
      "30295\n",
      "Predicted: \"[6]\", Actual: \"2\"\n",
      "27445\n",
      "Predicted: \"[13, 17]\", Actual: \"18\"\n",
      "16215\n",
      "Predicted: \"[13, 6]\", Actual: \"6\"\n",
      "35579\n",
      "Predicted: \"[12]\", Actual: \"0\"\n",
      "59131\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "26178\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "59642\n",
      "Predicted: \"[17, 7]\", Actual: \"2\"\n",
      "26026\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "34170\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "84536\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "46559\n",
      "Predicted: \"[7]\", Actual: \"5\"\n",
      "70201\n",
      "Predicted: \"[7, 12]\", Actual: \"1\"\n",
      "5027\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "68695\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "5899\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "81499\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "78222\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "60985\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "62162\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "53782\n",
      "Predicted: \"[13]\", Actual: \"13\"\n",
      "72487\n",
      "Predicted: \"[6]\", Actual: \"6\"\n",
      "75428\n",
      "Predicted: \"[11, 6]\", Actual: \"22\"\n",
      "87551\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "17632\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "25449\n",
      "Predicted: \"[13, 17]\", Actual: \"13\"\n",
      "9958\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "50283\n",
      "Predicted: \"[6, 12]\", Actual: \"0\"\n",
      "70369\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "81869\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "79270\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "17806\n",
      "Predicted: \"[7]\", Actual: \"22\"\n",
      "70178\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "72426\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "11308\n",
      "Predicted: \"[13]\", Actual: \"13\"\n",
      "73277\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "44179\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "39023\n",
      "Predicted: \"[13]\", Actual: \"13\"\n",
      "33204\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "68767\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "65079\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "48719\n",
      "Predicted: \"[6]\", Actual: \"11\"\n",
      "86438\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "80302\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "63117\n",
      "Predicted: \"[11]\", Actual: \"8\"\n",
      "8854\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "62337\n",
      "Predicted: \"[7]\", Actual: \"14\"\n",
      "30108\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "16567\n",
      "Predicted: \"[6, 13]\", Actual: \"6\"\n",
      "79292\n",
      "Predicted: \"[6]\", Actual: \"5\"\n",
      "74187\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "31275\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "33312\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "47679\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "4970\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "23139\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "1627\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "81977\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "40052\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "41400\n",
      "Predicted: \"[6, 11]\", Actual: \"16\"\n",
      "23232\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "9976\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "755\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "24140\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "79099\n",
      "Predicted: \"[2, 0]\", Actual: \"1\"\n",
      "2616\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "78452\n",
      "Predicted: \"[13, 6]\", Actual: \"13\"\n",
      "81991\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "27257\n",
      "Predicted: \"[6]\", Actual: \"1\"\n",
      "52679\n",
      "Predicted: \"[6, 0]\", Actual: \"6\"\n",
      "13999\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "58786\n",
      "Predicted: \"[6, 17]\", Actual: \"6\"\n",
      "61002\n",
      "Predicted: \"[6, 12]\", Actual: \"12\"\n",
      "13536\n",
      "Predicted: \"[17]\", Actual: \"17\"\n",
      "87299\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "68781\n",
      "Predicted: \"[17]\", Actual: \"1\"\n",
      "61669\n",
      "Predicted: \"[13, 17]\", Actual: \"7\"\n",
      "19736\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "31490\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "52466\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "59033\n",
      "Predicted: \"[17]\", Actual: \"2\"\n",
      "46177\n",
      "Predicted: \"[11]\", Actual: \"11\"\n",
      "22480\n",
      "Predicted: \"[6]\", Actual: \"2\"\n",
      "55968\n",
      "Predicted: \"[12]\", Actual: \"12\"\n",
      "5731\n",
      "Predicted: \"[12]\", Actual: \"8\"\n",
      "15755\n",
      "Predicted: \"[17, 11]\", Actual: \"8\"\n",
      "80488\n",
      "Predicted: \"[17]\", Actual: \"13\"\n",
      "21553\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "7385\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "52481\n",
      "Predicted: \"[11]\", Actual: \"8\"\n",
      "53894\n",
      "Predicted: \"[13, 6]\", Actual: \"4\"\n",
      "23290\n",
      "Predicted: \"[7]\", Actual: \"7\"\n",
      "87325\n",
      "Predicted: \"[11]\", Actual: \"11\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create new array with 60% of the data\n",
    "train_size = int(0.6 * len(accident_dataset))\n",
    "test_size = len(accident_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(accident_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(accident_dataset))\n",
    "\n",
    "#Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, 64)\n",
    "test_dataloader = DataLoader(test_dataset, 64)\n",
    "\n",
    "# define the class for multilinear regression\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(grid_size ** 2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, grid_size ** 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# define the class for multilinear regression\n",
    "# building the model object\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = Network().to(device)\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    print(\"Loaded model from model.pth\")\n",
    "else:\n",
    "    print(\"No model found, creating new model\")\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define the training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    model.train()\n",
    "    print(\"Training model\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        #print('X ', X)\n",
    "        #print('y ', y)\n",
    "        #print (X.shape)\n",
    "        pred = model(X)\n",
    "        #print('pred ', pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(\"Finished training model\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    print(\"Testing model\")\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, also_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #print('y ', y)\n",
    "            #print('predition', pred.argmax(1))\n",
    "\n",
    "            #check if prediction is correct\n",
    "            predictions = torch.topk(pred, 5, dim=1).indices\n",
    "            #is_correct = (pred.argmax(1) == y or pred.argmax(1) == max_value)\n",
    "\n",
    "            for i in range (len(predictions)):\n",
    "                if y[i] in predictions[i]:\n",
    "                    if y[i] == pred.argmax(1)[i]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        also_correct += 1\n",
    "\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            #print(correct)\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Main correct: {correct}  size: {size}  main correct/size: {correct/size}\")\n",
    "    print(f\"Also correct: {also_correct}  size: {size}  also correct/size: {also_correct/size}\")\n",
    "    correct += also_correct\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "epochs = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model.eval()\n",
    "for i in range (100):\n",
    "    randomnumber = rand.randint(0, len(test_dataset) - 1)\n",
    "    print(randomnumber)\n",
    "    x, y = test_dataset[randomnumber][0], test_dataset[randomnumber][1]\n",
    "    #x = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "    #x = torch.tensor(x).float()\n",
    "    #y = 0\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.to(device))\n",
    "        #print(pred)\n",
    "        predicted, actual = pred.topk(grid_size), y\n",
    "        max_value = pred.max(0)[0]\n",
    "        index = []\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted.values[i].item() >= 0.8 * max_value:\n",
    "                index.append(predicted.indices[i].item())\n",
    "        print(f'Predicted: \"{index}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "570189b94c8be545687b2cc37d4a9df3fcede358db47b55e6620cb36780e1fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
