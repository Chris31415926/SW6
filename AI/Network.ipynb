{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9892de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1471978380.0, '40.743782', '-73.97352']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import Data.database_handler as dbHandler\n",
    "from torchvision import transforms, utils\n",
    "import datetime as dt\n",
    "sys.path.append('..')\n",
    "#%run Map_grid/map.ipynb import CalculateGrid\n",
    "\n",
    "#Connecting to the SQLite database\n",
    "data_amount = 1500000\n",
    "db_path = r'Data\\datasetNY.db'\n",
    "grid_size = 5\n",
    "chunk_amount = 225555\n",
    "chunk_size = data_amount / chunk_amount\n",
    "data = dbHandler.get_n_data_datetime_converted(db_path, data_amount)\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.coordinates = data\n",
    "        self.coordinates = pd.DataFrame(self.coordinates, columns=['datetime', 'latitude', 'longitude'])\n",
    "        \n",
    "        #split into 500 chunks using numpy\n",
    "        self.coordinates = np.array_split(self.coordinates, chunk_amount)\n",
    "\n",
    "        #process each chunk and merge it back into one dataframe\n",
    "        self.grids = []\n",
    "        grid_lower_lat, grid_lower_long = 40.54, -74.15\n",
    "        grid_upper_lat, grid_upper_long = 40.91, -73.70\n",
    "        grid_lat_step = (grid_upper_lat - grid_lower_lat) / grid_size\n",
    "        grid_long_step = (grid_upper_long - grid_lower_long) / grid_size\n",
    "        for i in range(len(self.coordinates)-1):\n",
    "            grid = np.zeros((grid_size, grid_size))\n",
    "            for index, row in self.coordinates[i].iterrows():\n",
    "                coordinates = row['latitude'], row['longitude']\n",
    "                for j in range(grid_size):\n",
    "                    for k in range(grid_size):\n",
    "                        lat_lower = grid_lower_lat + j * grid_lat_step\n",
    "                        lat_upper = grid_lower_lat + (j + 1) * grid_lat_step\n",
    "                        long_lower = grid_lower_long + k * grid_long_step\n",
    "                        long_upper = grid_lower_long + (k + 1) * grid_long_step\n",
    "                        if lat_lower <= float(coordinates[0]) < lat_upper and long_lower <= float(coordinates[1]) < long_upper:\n",
    "                            grid[j][k] += 1\n",
    "                            break\n",
    "            self.grids.append(grid/chunk_size)\n",
    "        self.grids = np.array(self.grids)\n",
    "        self.transform = transform      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        grid = self.grids[idx]\n",
    "        grid = torch.from_numpy(grid).float()\n",
    "\n",
    "        max_index = np.argmax(grid)\n",
    "        max_index = np.array(max_index)\n",
    "        #get indicies of the highest value and nearest neighbours that are within 10% of the highest value\n",
    "        x_and_y = max_index.item() // grid_size, max_index.item() % grid_size\n",
    "        max_value = grid[x_and_y[0]][x_and_y[1]].item()\n",
    "        amount = 0\n",
    "        indicies = np.empty((0,2), int)\n",
    "        for i in range(grid_size):\n",
    "            for j in range(grid_size):\n",
    "                if grid[i][j].item() >= max_value * 0.9:\n",
    "                    indicies = np.append(indicies, i * grid_size + j)\n",
    "                    amount += 1\n",
    "\n",
    "        indicies = torch.from_numpy(indicies).reshape(-1, 1).flatten().long()\n",
    "        indicies = torch.nn.functional.pad(indicies, (0, grid_size - len(indicies)), value=0)\n",
    "        #get amount of indicies as a tensor\n",
    "        indicies_amount = torch.tensor(amount-1).long()\n",
    "        return grid.flatten(), torch.tensor(max_index.item()).long()\n",
    "\n",
    "accident_dataset = AccidentDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f366a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135332\n",
      "90222\n",
      "225554\n",
      "Using cpu device\n",
      "Loaded model from model.pth\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "135332\n",
      "Training model\n",
      "loss: 1.483952  [   64/135332]\n",
      "loss: 1.022654  [ 6464/135332]\n",
      "loss: 1.333812  [12864/135332]\n",
      "loss: 1.255050  [19264/135332]\n",
      "loss: 1.158978  [25664/135332]\n",
      "loss: 1.370076  [32064/135332]\n",
      "loss: 1.392652  [38464/135332]\n",
      "loss: 1.257803  [44864/135332]\n",
      "loss: 1.135891  [51264/135332]\n",
      "loss: 1.022093  [57664/135332]\n",
      "loss: 1.083316  [64064/135332]\n",
      "loss: 1.077167  [70464/135332]\n",
      "loss: 1.372873  [76864/135332]\n",
      "loss: 1.125708  [83264/135332]\n",
      "loss: 1.179053  [89664/135332]\n",
      "loss: 1.190702  [96064/135332]\n",
      "loss: 1.304964  [102464/135332]\n",
      "loss: 1.473787  [108864/135332]\n",
      "loss: 1.493595  [115264/135332]\n",
      "loss: 1.332384  [121664/135332]\n",
      "loss: 1.194142  [128064/135332]\n",
      "loss: 1.227458  [134464/135332]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 60437  size: 90222  main correct/size: 0.6698698765267894\n",
      "Also correct: 15872  size: 90222  also correct/size: 0.1759216155704817\n",
      "Test Error: Accuracy: 84.6%, Avg loss: 1.233250\n",
      "Saved PyTorch Model State to model.pth\n",
      "tensor([ 2.2385,  2.2365,  2.6476, -0.1227,  0.3029,  1.2437,  2.4701, -0.4188,\n",
      "         1.6847,  0.7509, -3.1403,  0.2642, -4.4262,  3.2002,  1.9141, -4.1871,\n",
      "         0.3713,  3.8928,  0.8943, -3.0241, -4.0066, -3.7542,  1.3458,  1.2745,\n",
      "        -3.6115])\n",
      "Predicted: \"[17, 13]\", Actual: \"17\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create new array with 60% of the data\n",
    "train_size = int(0.6 * len(accident_dataset))\n",
    "test_size = len(accident_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(accident_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(accident_dataset))\n",
    "\n",
    "#Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, 64)\n",
    "test_dataloader = DataLoader(test_dataset, 64)\n",
    "\n",
    "# define the class for multilinear regression\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(grid_size ** 2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, grid_size ** 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# define the class for multilinear regression\n",
    "# building the model object\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = Network().to(device)\n",
    "if os.path.exists(\"model.pth\"):\n",
    "    model.load_state_dict(torch.load(\"model.pth\"))\n",
    "    print(\"Loaded model from model.pth\")\n",
    "else:\n",
    "    print(\"No model found, creating new model\")\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define the training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    model.train()\n",
    "    print(\"Training model\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        #print('X ', X)\n",
    "        #print('y ', y)\n",
    "        #print (X.shape)\n",
    "        pred = model(X)\n",
    "        #print('pred ', pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(\"Finished training model\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    print(\"Testing model\")\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, also_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #print('y ', y)\n",
    "            #print('predition', pred.argmax(1))\n",
    "\n",
    "            #check if prediction is correct\n",
    "            predictions = torch.topk(pred, 5, dim=1).indices\n",
    "            #is_correct = (pred.argmax(1) == y or pred.argmax(1) == max_value)\n",
    "\n",
    "            for i in range (len(predictions)):\n",
    "                if y[i] in predictions[i]:\n",
    "                    if y[i] == pred.argmax(1)[i]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        also_correct += 1\n",
    "\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            #print(correct)\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Main correct: {correct}  size: {size}  main correct/size: {correct/size}\")\n",
    "    print(f\"Also correct: {also_correct}  size: {size}  also correct/size: {also_correct/size}\")\n",
    "    correct += also_correct\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "epochs = 1\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(\"Saved PyTorch Model State to model.pth\")\n",
    "\n",
    "model.eval()\n",
    "x, y = test_dataset[0][0], test_dataset[0][1]\n",
    "#x = [1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]\n",
    "#x = torch.tensor(x).float()\n",
    "#y = 0\n",
    "with torch.no_grad():\n",
    "    pred = model(x.to(device))\n",
    "    print(pred)\n",
    "    predicted, actual = pred.topk(grid_size), y\n",
    "    max_value = pred.max(0)[0]\n",
    "    index = []\n",
    "    for i in range(len(predicted)):\n",
    "        if predicted.values[i].item() >= 0.8 * max_value:\n",
    "            index.append(predicted.indices[i].item())\n",
    "    print(f'Predicted: \"{index}\", Actual: \"{actual}\"')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "570189b94c8be545687b2cc37d4a9df3fcede358db47b55e6620cb36780e1fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
