{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb9892de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1450620000.0, '40.6720753', '-73.9113364']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import Data.database_handler as dbHandler\n",
    "from torchvision import transforms, utils\n",
    "import datetime as dt\n",
    "import random as rand\n",
    "sys.path.append('..')\n",
    "#%run Map_grid/map.ipynb import CalculateGrid\n",
    "\n",
    "#Connecting to the SQLite database\n",
    "data_amount = 1600000\n",
    "db_path = r'Data\\datasetNY.db'\n",
    "grid_size = 5\n",
    "chunk_amount = 85555\n",
    "chunk_size = data_amount / chunk_amount\n",
    "data = dbHandler.get_n_data_datetime_converted(db_path, data_amount)\n",
    "\n",
    "class AccidentDataset(Dataset):\n",
    "    def __init__(self, transform=None):\n",
    "        self.coordinates = data\n",
    "        self.coordinates = pd.DataFrame(self.coordinates, columns=['datetime', 'latitude', 'longitude'])\n",
    "        \n",
    "        #split into 500 chunks using numpy\n",
    "        self.coordinates = np.array_split(self.coordinates, chunk_amount)\n",
    "\n",
    "        #process each chunk and merge it back into one dataframe\n",
    "        self.grids = []\n",
    "        grid_lower_lat, grid_lower_long = 40.54, -74.15\n",
    "        grid_upper_lat, grid_upper_long = 40.91, -73.70\n",
    "        grid_lat_step = (grid_upper_lat - grid_lower_lat) / grid_size\n",
    "        grid_long_step = (grid_upper_long - grid_lower_long) / grid_size\n",
    "        for i in range(len(self.coordinates)-1):\n",
    "            grid = np.zeros((grid_size, grid_size))\n",
    "            for index, row in self.coordinates[i].iterrows():\n",
    "                coordinates = row['latitude'], row['longitude']\n",
    "                for j in range(grid_size):\n",
    "                    for k in range(grid_size):\n",
    "                        lat_lower = grid_lower_lat + j * grid_lat_step\n",
    "                        lat_upper = grid_lower_lat + (j + 1) * grid_lat_step\n",
    "                        long_lower = grid_lower_long + k * grid_long_step\n",
    "                        long_upper = grid_lower_long + (k + 1) * grid_long_step\n",
    "                        if lat_lower <= float(coordinates[0]) < lat_upper and long_lower <= float(coordinates[1]) < long_upper:\n",
    "                            grid[j][k] += 1\n",
    "                            break\n",
    "            self.grids.append(grid/chunk_size)\n",
    "        self.grids = np.array(self.grids)\n",
    "        self.transform = transform      \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.grids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        grid = self.grids[idx]\n",
    "        grid = torch.from_numpy(grid).float()\n",
    "\n",
    "        max_index = np.argmax(grid)\n",
    "        max_index = np.array(max_index)\n",
    "        return grid.flatten(), torch.tensor(max_index.item()).long()\n",
    "\n",
    "accident_dataset = AccidentDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f366a4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39332\n",
      "26222\n",
      "65554\n",
      "Using cpu device\n",
      "Loaded model from model5.pth\n",
      "Saved PyTorch Model State to model.pth\n",
      "Predicted: [0, 8]  Actual: 0  True   9.51073932647705\n",
      "Predicted: [22]    Actual: 1  False  13.900676727294922\n",
      "Predicted: [2, 14] Actual: 2  True   11.776857376098633\n",
      "Predicted: [8, 14] Actual: 3  False  4.744635105133057\n",
      "Predicted: [22, 1] Actual: 4  False  5.104641437530518\n",
      "Predicted: [14, 0] Actual: 5  False  5.133382797241211\n",
      "Predicted: [6]     Actual: 6  True   43.534427642822266\n",
      "Predicted: [7]     Actual: 7  True   46.286895751953125\n",
      "Predicted: [8]     Actual: 8  True   9.988083839416504\n",
      "Predicted: [22, 8] Actual: 9  False  5.505378723144531\n",
      "Predicted: [22, 8] Actual: 10 False  3.694668769836426\n",
      "Predicted: [11]    Actual: 11 True   43.805519104003906\n",
      "Predicted: [12]    Actual: 12 True   34.814300537109375\n",
      "Predicted: [13]    Actual: 13 True   33.068626403808594\n",
      "Predicted: [14, 2] Actual: 14 True   6.909416675567627\n",
      "Predicted: [11, 8] Actual: 15 False  3.640035629272461\n",
      "Predicted: [8, 6]  Actual: 16 False  2.996849298477173\n",
      "Predicted: [17]    Actual: 17 True   39.80656433105469\n",
      "Predicted: [8, 22] Actual: 18 False  3.268179416656494\n",
      "Predicted: [7, 8]  Actual: 19 False  4.080883502960205\n",
      "Predicted: [7]     Actual: 20 False  5.697951793670654\n",
      "Predicted: [22]    Actual: 21 False  5.490937232971191\n",
      "Predicted: [22]    Actual: 22 True   16.832107543945312\n",
      "Predicted: [23, 8] Actual: 23 True   5.434359073638916\n",
      "Predicted: [8, 14] Actual: 24 False  3.949232339859009\n",
      "------------------------------------------\n",
      "Edge correct: 12   Size: 25   Edge correct/Size: 0.48\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Create new array with 60% of the data\n",
    "train_size = int(0.6 * len(accident_dataset))\n",
    "test_size = len(accident_dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(accident_dataset, [train_size, test_size])\n",
    "\n",
    "print(len(train_dataset))\n",
    "print(len(test_dataset))\n",
    "print(len(accident_dataset))\n",
    "\n",
    "#Create dataloader\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "# define the class for multilinear regression\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(grid_size ** 2, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, 25),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(25, grid_size ** 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        #x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "# define the class for multilinear regression\n",
    "# building the model object\n",
    "#device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device = torch.device('cpu')\n",
    "print(f'Using {device} device')\n",
    "\n",
    "model = Network().to(device)\n",
    "if os.path.exists(f\"model{grid_size}.pth\"):\n",
    "    model.load_state_dict(torch.load(f\"model{grid_size}.pth\"))\n",
    "    print(f\"Loaded model from model{grid_size}.pth\")\n",
    "else:\n",
    "    print(\"No model found, creating new model\")\n",
    "\n",
    "# define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# define the training loop\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    print(size)\n",
    "    model.train()\n",
    "    print(\"Training model\")\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        #print('pred ', pred)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    print(\"Finished training model\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    print(\"Testing model\")\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct, also_correct = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            #print('y ', y)\n",
    "            #print('predition', pred.argmax(1))\n",
    "\n",
    "            #check if prediction is correct\n",
    "            predictions = torch.topk(pred, 2, dim=1).indices\n",
    "            #is_correct = (pred.argmax(1) == y or pred.argmax(1) == max_value)\n",
    "\n",
    "            for i in range (len(predictions)):\n",
    "                if y[i] in predictions[i]:\n",
    "                    if y[i] == pred.argmax(1)[i]:\n",
    "                        correct += 1\n",
    "                    else:\n",
    "                        also_correct += 1\n",
    "\n",
    "            #correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "            #print(correct)\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Main correct: {correct}  size: {size}  main correct/size: {correct/size}\")\n",
    "    print(f\"Also correct: {also_correct}  size: {size}  also correct/size: {also_correct/size}\")\n",
    "    correct += also_correct\n",
    "    correct /= size\n",
    "    print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "epochs = 0\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "\n",
    "torch.save(model.state_dict(), f\"model{grid_size}.pth\")\n",
    "print(f\"Saved PyTorch Model State to model{grid_size}.pth\")\n",
    "\n",
    "model.eval()\n",
    "edge_correct = 0\n",
    "for i in range (grid_size ** 2):\n",
    "    randomnumber = rand.randint(0, len(test_dataset) - 1)\n",
    "    #randomnumber = 86903\n",
    "    #print(randomnumber)\n",
    "    #x, y = test_dataset[randomnumber][0], test_dataset[randomnumber][1]\n",
    "    edge = np.zeros(grid_size ** 2)\n",
    "    edge[i] = 1\n",
    "    x, y = torch.tensor(edge).float(), i\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(x.to(device))\n",
    "        #print(pred)\n",
    "        predicted, actual = pred.topk(grid_size), y\n",
    "        max_value = pred.max(0)[0]\n",
    "        index = []\n",
    "        for i in range(len(predicted)):\n",
    "            if predicted.values[i].item() >= 0.8 * max_value:\n",
    "                index.append(predicted.indices[i].item())\n",
    "        part1 = f'Predicted: {index}'.ljust(18, ' ')\n",
    "        part2 = f'Actual: {actual}'.ljust(10, ' ')\n",
    "        part3 = f'{actual in index}'.ljust(6, ' ')\n",
    "        part4 = f'{max_value}'.ljust(10, ' ')\n",
    "        print(part1, part2, part3, part4)\n",
    "        #print(f'Predicted: \"{index}\", Actual: \"{actual}\" {actual in index} {max_value}')\n",
    "        edge_correct += actual in index\n",
    "print('------------------------------------------')\n",
    "edgestr1 = f\"Edge correct: {edge_correct}\".ljust(18, ' ')\n",
    "edgestr2 = f\"Size: {grid_size ** 2}\".ljust(10, ' ')\n",
    "edgestr3 = f\"Edge correct/Size: {edge_correct/(grid_size ** 2)}\".ljust(20, ' ')\n",
    "print(edgestr1, edgestr2, edgestr3)\n",
    "#print(f\"Edge correct: {edge_correct}  size: {25}  edge correct/size: {edge_correct/25}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4d3e1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1506533400.0, '40.777958', '-73.90847']\n",
      "Grid size: 5\n",
      "120132\n",
      "80089\n",
      "200221\n",
      "Using cpu device\n",
      "No model found, creating new model\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "120132\n",
      "Training model\n",
      "loss: 3.213377  [   64/120132]\n",
      "loss: 3.209886  [ 6464/120132]\n",
      "loss: 3.183859  [12864/120132]\n",
      "loss: 3.179130  [19264/120132]\n",
      "loss: 3.182967  [25664/120132]\n",
      "loss: 3.113484  [32064/120132]\n",
      "loss: 3.152517  [38464/120132]\n",
      "loss: 3.132189  [44864/120132]\n",
      "loss: 3.080526  [51264/120132]\n",
      "loss: 3.075877  [57664/120132]\n",
      "loss: 3.077132  [64064/120132]\n",
      "loss: 3.040312  [70464/120132]\n",
      "loss: 3.038178  [76864/120132]\n",
      "loss: 3.077791  [83264/120132]\n",
      "loss: 3.039606  [89664/120132]\n",
      "loss: 3.078412  [96064/120132]\n",
      "loss: 3.038011  [102464/120132]\n",
      "loss: 3.065795  [108864/120132]\n",
      "loss: 3.016800  [115264/120132]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 17029  size: 80089  main correct/size: 0.21262595362659042\n",
      "Also correct: 13881  size: 80089  also correct/size: 0.1733196818539375\n",
      "Test Error: Accuracy: 38.6%, Avg loss: 2.991716\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "120132\n",
      "Training model\n",
      "loss: 3.065110  [   64/120132]\n",
      "loss: 3.043092  [ 6464/120132]\n",
      "loss: 2.995110  [12864/120132]\n",
      "loss: 3.001025  [19264/120132]\n",
      "loss: 3.023528  [25664/120132]\n",
      "loss: 2.919088  [32064/120132]\n",
      "loss: 2.995110  [38464/120132]\n",
      "loss: 2.957959  [44864/120132]\n",
      "loss: 2.874606  [51264/120132]\n",
      "loss: 2.861134  [57664/120132]\n",
      "loss: 2.894362  [64064/120132]\n",
      "loss: 2.829432  [70464/120132]\n",
      "loss: 2.836471  [76864/120132]\n",
      "loss: 2.918388  [83264/120132]\n",
      "loss: 2.840970  [89664/120132]\n",
      "loss: 2.926356  [96064/120132]\n",
      "loss: 2.854627  [102464/120132]\n",
      "loss: 2.909910  [108864/120132]\n",
      "loss: 2.829449  [115264/120132]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 17029  size: 80089  main correct/size: 0.21262595362659042\n",
      "Also correct: 13881  size: 80089  also correct/size: 0.1733196818539375\n",
      "Test Error: Accuracy: 38.6%, Avg loss: 2.804641\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "120132\n",
      "Training model\n",
      "loss: 2.933963  [   64/120132]\n",
      "loss: 2.891276  [ 6464/120132]\n",
      "loss: 2.816165  [12864/120132]\n",
      "loss: 2.832590  [19264/120132]\n",
      "loss: 2.876341  [25664/120132]\n",
      "loss: 2.732940  [32064/120132]\n",
      "loss: 2.848789  [38464/120132]\n",
      "loss: 2.788147  [44864/120132]\n",
      "loss: 2.669757  [51264/120132]\n",
      "loss: 2.647825  [57664/120132]\n",
      "loss: 2.717658  [64064/120132]\n",
      "loss: 2.624010  [70464/120132]\n",
      "loss: 2.641756  [76864/120132]\n",
      "loss: 2.773031  [83264/120132]\n",
      "loss: 2.646492  [89664/120132]\n",
      "loss: 2.794355  [96064/120132]\n",
      "loss: 2.683064  [102464/120132]\n",
      "loss: 2.772470  [108864/120132]\n",
      "loss: 2.652128  [115264/120132]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 17029  size: 80089  main correct/size: 0.21262595362659042\n",
      "Also correct: 13881  size: 80089  also correct/size: 0.1733196818539375\n",
      "Test Error: Accuracy: 38.6%, Avg loss: 2.634201\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "120132\n",
      "Training model\n",
      "loss: 2.826802  [   64/120132]\n",
      "loss: 2.761024  [ 6464/120132]\n",
      "loss: 2.664569  [12864/120132]\n",
      "loss: 2.693304  [19264/120132]\n",
      "loss: 2.754127  [25664/120132]\n",
      "loss: 2.584313  [32064/120132]\n",
      "loss: 2.735085  [38464/120132]\n",
      "loss: 2.640292  [44864/120132]\n",
      "loss: 2.496057  [51264/120132]\n",
      "loss: 2.472904  [57664/120132]\n",
      "loss: 2.576885  [64064/120132]\n",
      "loss: 2.471718  [70464/120132]\n",
      "loss: 2.498007  [76864/120132]\n",
      "loss: 2.676784  [83264/120132]\n",
      "loss: 2.493145  [89664/120132]\n",
      "loss: 2.715497  [96064/120132]\n",
      "loss: 2.558973  [102464/120132]\n",
      "loss: 2.684293  [108864/120132]\n",
      "loss: 2.517194  [115264/120132]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 17029  size: 80089  main correct/size: 0.21262595362659042\n",
      "Also correct: 13881  size: 80089  also correct/size: 0.1733196818539375\n",
      "Test Error: Accuracy: 38.6%, Avg loss: 2.519449\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "120132\n",
      "Training model\n",
      "loss: 2.767450  [   64/120132]\n",
      "loss: 2.675408  [ 6464/120132]\n",
      "loss: 2.578481  [12864/120132]\n",
      "loss: 2.615799  [19264/120132]\n",
      "loss: 2.673577  [25664/120132]\n",
      "loss: 2.511095  [32064/120132]\n",
      "loss: 2.671919  [38464/120132]\n",
      "loss: 2.535882  [44864/120132]\n",
      "loss: 2.386379  [51264/120132]\n",
      "loss: 2.371094  [57664/120132]\n",
      "loss: 2.488728  [64064/120132]\n",
      "loss: 2.396694  [70464/120132]\n",
      "loss: 2.425963  [76864/120132]\n",
      "loss: 2.633510  [83264/120132]\n",
      "loss: 2.398317  [89664/120132]\n",
      "loss: 2.682089  [96064/120132]\n",
      "loss: 2.486445  [102464/120132]\n",
      "loss: 2.638924  [108864/120132]\n",
      "loss: 2.429366  [115264/120132]\n",
      "Finished training model\n",
      "Testing model\n",
      "Main correct: 17029  size: 80089  main correct/size: 0.21262595362659042\n",
      "Also correct: 13881  size: 80089  also correct/size: 0.1733196818539375\n",
      "Test Error: Accuracy: 38.6%, Avg loss: 2.459304\n",
      "Testing model\n",
      "Saved PyTorch Model State to model5.pth\n",
      "------------------------------------------\n",
      "Edge cases:\n",
      "Predicted: [7, 11] Actual: 0  False  2.204423427581787\n",
      "Predicted: [7, 11] Actual: 1  False  2.2070438861846924\n",
      "Predicted: [7, 11] Actual: 2  False  2.129917621612549\n",
      "Predicted: [7, 11] Actual: 3  False  2.2204155921936035\n",
      "Predicted: [7, 11] Actual: 4  False  2.1749043464660645\n",
      "Predicted: [7, 11] Actual: 5  False  2.1728413105010986\n",
      "Predicted: [7, 11] Actual: 6  False  2.1351380348205566\n",
      "Predicted: [7, 11] Actual: 7  True   2.2819271087646484\n",
      "Predicted: [7, 11] Actual: 8  False  2.200652599334717\n",
      "Predicted: [7, 11] Actual: 9  False  2.2314631938934326\n",
      "Predicted: [7, 11] Actual: 10 False  2.265756845474243\n",
      "Predicted: [7, 11] Actual: 11 True   2.216718912124634\n",
      "Predicted: [7, 11] Actual: 12 False  2.2317352294921875\n",
      "Predicted: [7, 11] Actual: 13 False  2.109853506088257\n",
      "Predicted: [7, 11] Actual: 14 False  2.1956536769866943\n",
      "Predicted: [7, 11] Actual: 15 False  2.193070888519287\n",
      "Predicted: [7, 11] Actual: 16 False  2.1393136978149414\n",
      "Predicted: [7, 11] Actual: 17 False  2.1852266788482666\n",
      "Predicted: [7, 11] Actual: 18 False  2.148757219314575\n",
      "Predicted: [7, 11] Actual: 19 False  2.0963168144226074\n",
      "Predicted: [7, 11] Actual: 20 False  2.2328195571899414\n",
      "Predicted: [7, 11] Actual: 21 False  2.2105116844177246\n",
      "Predicted: [7, 11] Actual: 22 False  2.261836528778076\n",
      "Predicted: [7, 11] Actual: 23 False  2.1917288303375244\n",
      "Predicted: [7, 11] Actual: 24 False  2.121760129928589\n",
      "Edge correct: 2    Size: 25   Edge correct/Size: 0.08\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import sqlite3\n",
    "import sys\n",
    "import traceback\n",
    "import numpy as np\n",
    "import Data.database_handler as dbHandler\n",
    "from torchvision import transforms, utils\n",
    "import datetime as dt\n",
    "import random as rand\n",
    "sys.path.append('..')\n",
    "#%run Map_grid/map.ipynb import CalculateGrid\n",
    "\n",
    "#Connecting to the SQLite database\n",
    "grid_size = 5\n",
    "data_amount = 1600000\n",
    "db_path = r'Data\\datasetNY.db'\n",
    "chunk_amount = 200222\n",
    "chunk_size = data_amount / chunk_amount\n",
    "data = dbHandler.get_n_data_datetime_converted(db_path, data_amount)\n",
    "\n",
    "with open('output2.txt', 'a') as f:\n",
    "    for size in range(5, 6):\n",
    "        grid_size = size\n",
    "        print(f'Grid size: {grid_size}')\n",
    "        f.write(f'Grid size: {grid_size}\\n')\n",
    "        f.write(f'Correct Tolerance {int(max(1, np.floor(grid_size/2)))}\\n')\n",
    "        f.write('------------------------------------------\\n')\n",
    "        class AccidentDataset(Dataset):\n",
    "            def __init__(self, transform=None):\n",
    "                self.coordinates = data\n",
    "                self.coordinates = pd.DataFrame(self.coordinates, columns=['datetime', 'latitude', 'longitude'])\n",
    "                \n",
    "                #split into 500 chunks using numpy\n",
    "                self.coordinates = np.array_split(self.coordinates, chunk_amount)\n",
    "\n",
    "                #process each chunk and merge it back into one dataframe\n",
    "                self.grids = []\n",
    "                grid_lower_lat, grid_lower_long = 40.54, -74.15\n",
    "                grid_upper_lat, grid_upper_long = 40.91, -73.70\n",
    "                grid_lat_step = (grid_upper_lat - grid_lower_lat) / grid_size\n",
    "                grid_long_step = (grid_upper_long - grid_lower_long) / grid_size\n",
    "                for i in range(len(self.coordinates)-1):\n",
    "                    grid = np.zeros((grid_size, grid_size))\n",
    "                    for index, row in self.coordinates[i].iterrows():\n",
    "                        coordinates = row['latitude'], row['longitude']\n",
    "                        for j in range(grid_size):\n",
    "                            for k in range(grid_size):\n",
    "                                lat_lower = grid_lower_lat + j * grid_lat_step\n",
    "                                lat_upper = grid_lower_lat + (j + 1) * grid_lat_step\n",
    "                                long_lower = grid_lower_long + k * grid_long_step\n",
    "                                long_upper = grid_lower_long + (k + 1) * grid_long_step\n",
    "                                if lat_lower <= float(coordinates[0]) < lat_upper and long_lower <= float(coordinates[1]) < long_upper:\n",
    "                                    grid[j][k] += 1\n",
    "                                    break\n",
    "                    self.grids.append(grid/chunk_size)\n",
    "                self.grids = np.array(self.grids)\n",
    "                self.transform = transform      \n",
    "\n",
    "            def __len__(self):\n",
    "                return len(self.grids)\n",
    "            \n",
    "            def __getitem__(self, idx):\n",
    "                if torch.is_tensor(idx):\n",
    "                    idx = idx.tolist()\n",
    "\n",
    "                grid = self.grids[idx]\n",
    "                grid = torch.from_numpy(grid).float()\n",
    "\n",
    "                max_index = np.argmax(grid)\n",
    "                max_index = np.array(max_index)\n",
    "                return grid.flatten(), torch.tensor(max_index.item()).long()\n",
    "\n",
    "        accident_dataset = AccidentDataset()\n",
    "\n",
    "        #Create new array with 60% of the data\n",
    "        train_size = int(0.6 * len(accident_dataset))\n",
    "        test_size = len(accident_dataset) - train_size\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(accident_dataset, [train_size, test_size])\n",
    "\n",
    "        print(len(train_dataset))\n",
    "        print(len(test_dataset))\n",
    "        print(len(accident_dataset))\n",
    "\n",
    "        #Create dataloader\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=64)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "        # define the class for multilinear regression\n",
    "        class Network(torch.nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.flatten = nn.Flatten()\n",
    "                self.dropout = nn.Dropout(0.2)\n",
    "                self.linear_relu_stack = nn.Sequential(\n",
    "                    nn.Linear(grid_size ** 2, grid_size ** 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(grid_size ** 2, grid_size ** 2),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(grid_size ** 2, grid_size ** 2),\n",
    "                )\n",
    "\n",
    "            def forward(self, x):\n",
    "                #x = self.flatten(x)\n",
    "                logits = self.linear_relu_stack(x)\n",
    "                return logits\n",
    "\n",
    "\n",
    "        # define the class for multilinear regression\n",
    "        # building the model object\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "        #device = torch.device('cpu')\n",
    "        print(f'Using {device} device')\n",
    "\n",
    "        model = Network().to(device)\n",
    "        if os.path.exists(f\"model{grid_size}.pth\"):\n",
    "            model.load_state_dict(torch.load(f\"model{grid_size}.pth\"))\n",
    "            print(f\"Loaded model from model{grid_size}.pth\")\n",
    "        else:\n",
    "            print(\"No model found, creating new model\")\n",
    "\n",
    "        # define the loss function\n",
    "        loss_fn = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "        # define the training loop\n",
    "        def train(dataloader, model, loss_fn, optimizer):\n",
    "            size = len(dataloader.dataset)\n",
    "            print(size)\n",
    "            model.train()\n",
    "            print(\"Training model\")\n",
    "            for batch, (X, y) in enumerate(dataloader):\n",
    "                X, y = X.to(device), y.to(device)\n",
    "\n",
    "                pred = model(X)\n",
    "                #print('pred ', pred)\n",
    "                loss = loss_fn(pred, y)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                if batch % 100 == 0:\n",
    "                    loss, current = loss.item(), (batch + 1) * len(X)\n",
    "                    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "            print(\"Finished training model\")\n",
    "\n",
    "        def test(dataloader, model, loss_fn):\n",
    "            print(\"Testing model\")\n",
    "            size = len(dataloader.dataset)\n",
    "            num_batches = len(dataloader)\n",
    "            model.eval()\n",
    "            test_loss, correct, also_correct = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in dataloader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    pred = model(X)\n",
    "                    test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "                    #check if prediction is correct\n",
    "                    predictions = torch.topk(pred, int(max(1, np.floor(grid_size/2))), dim=1).indices\n",
    "\n",
    "                    for i in range (len(predictions)):\n",
    "                        if y[i] in predictions[i]:\n",
    "                            if y[i] == pred.argmax(1)[i]:\n",
    "                                correct += 1\n",
    "                            else:\n",
    "                                also_correct += 1\n",
    "\n",
    "            test_loss /= num_batches\n",
    "            print(f\"Main correct: {correct}  size: {size}  main correct/size: {correct/size}\")\n",
    "            print(f\"Also correct: {also_correct}  size: {size}  also correct/size: {also_correct/size}\")\n",
    "            correct += also_correct\n",
    "            correct /= size\n",
    "            print(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "        def logtest(dataloader, model, loss_fn):\n",
    "            print(\"Testing model\")\n",
    "            size = len(dataloader.dataset)\n",
    "            num_batches = len(dataloader)\n",
    "            model.eval()\n",
    "            test_loss, correct, also_correct = 0, 0, 0\n",
    "            with torch.no_grad():\n",
    "                for X, y in dataloader:\n",
    "                    X, y = X.to(device), y.to(device)\n",
    "                    pred = model(X)\n",
    "                    test_loss += loss_fn(pred, y).item()\n",
    "\n",
    "                    #check if prediction is correct\n",
    "                    predictions = torch.topk(pred, int(max(1, np.floor(grid_size/2))), dim=1).indices\n",
    "\n",
    "                    for i in range (len(predictions)):\n",
    "                        if y[i] in predictions[i]:\n",
    "                            if y[i] == pred.argmax(1)[i]:\n",
    "                                correct += 1\n",
    "                            else:\n",
    "                                also_correct += 1\n",
    "\n",
    "            test_loss /= num_batches\n",
    "            f.write(f\"Main correct: {correct}  size: {size}  main correct/size: {correct/size}\\n\")\n",
    "            f.write(f\"Also correct: {also_correct}  size: {size}  also correct/size: {also_correct/size}\\n\")\n",
    "            correct += also_correct\n",
    "            correct /= size\n",
    "            f.write(f\"Test Error: Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\\n\")\n",
    "        \n",
    "        epochs = 5\n",
    "        for t in range(epochs):\n",
    "            print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "            train(train_dataloader, model, loss_fn, optimizer)\n",
    "            test(test_dataloader, model, loss_fn)\n",
    "\n",
    "        logtest(test_dataloader, model, loss_fn)\n",
    "        torch.save(model.state_dict(), f\"model{grid_size}.pth\")\n",
    "        print(f\"Saved PyTorch Model State to model{grid_size}.pth\")\n",
    "\n",
    "        model.eval()\n",
    "        edge_correct = 0\n",
    "        f.write('------------------------------------------\\nEdge cases:\\n')\n",
    "        print('------------------------------------------\\nEdge cases:')\n",
    "        for i in range (grid_size ** 2):\n",
    "            randomnumber = rand.randint(0, len(test_dataset) - 1)\n",
    "            edge = np.zeros(grid_size ** 2)\n",
    "            edge[i] = 1\n",
    "            x, y = torch.tensor(edge).float(), i\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                pred = model(x.to(device))\n",
    "                #print(pred)\n",
    "                predicted, actual = pred.topk(grid_size), y\n",
    "                max_value = pred.max(0)[0]\n",
    "                index = []\n",
    "                for j in range(len(predicted)):\n",
    "                    if predicted.values[j].item() >= 0.8 * max_value:\n",
    "                        index.append(predicted.indices[j].item())\n",
    "                part1 = f'Predicted: {index}'.ljust(18, ' ')\n",
    "                part2 = f'Actual: {actual}'.ljust(10, ' ')\n",
    "                part3 = f'{actual in index}'.ljust(6, ' ')\n",
    "                part4 = f'{max_value}'.ljust(10, ' ')\n",
    "                print(part1, part2, part3, part4)\n",
    "                f.write(part1 + part2 + part3 + part4 + '\\n')\n",
    "                #print(f'Predicted: \"{index}\", Actual: \"{actual}\" {actual in index} {max_value}')\n",
    "                edge_correct += actual in index\n",
    "        f.write('------------------------------------------\\n')\n",
    "        edgestr1 = f\"Edge correct: {edge_correct}\".ljust(18, ' ')\n",
    "        edgestr2 = f\"Size: {grid_size ** 2}\".ljust(10, ' ')\n",
    "        edgestr3 = f\"Edge correct/Size: {edge_correct/(grid_size ** 2)}\".ljust(20, ' ')\n",
    "        print(edgestr1, edgestr2, edgestr3)\n",
    "        f.write(edgestr1 + edgestr2 + edgestr3 + '\\n')\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "570189b94c8be545687b2cc37d4a9df3fcede358db47b55e6620cb36780e1fb9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
